{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Credit_card_aproval.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1_3bBhiCNtUYba6lfjZCNy-BwguBddjeV",
      "authorship_tag": "ABX9TyN9XVf94pvEdfwfJpzoZag/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IsaacKelly99/IKR/blob/master/Credit_card_aproval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieLqC3P6A3Ib"
      },
      "source": [
        "# Credit card aproval\n",
        "## ML supervised learning\n",
        "I built a model to predict whether a bank user will get an aproval on a new credit card or get rejected\n",
        "\n",
        "### Tools used:\n",
        "* Label encoder\n",
        "* Iterative imputation\n",
        "* train,test split\n",
        "* Standard scaler\n",
        "* Stacking classifier\n",
        "* metrics: accuracy_score, matthews_corrcoef, f1_score\n",
        "\n",
        "link to the dataset: http://archive.ics.uci.edu/ml/datasets/credit+approval"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzSgOF4nCC9k"
      },
      "source": [
        "# Packages\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "WoxVs_3jAwXh",
        "outputId": "a1a1ad64-e7dd-4b44-87f5-4426f182f779"
      },
      "source": [
        "#/content/drive/MyDrive/data_for_colab/crx.data\n",
        "df= pd.read_csv(\"/content/drive/MyDrive/data_for_colab/crx.data\", header=None)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>b</td>\n",
              "      <td>30.83</td>\n",
              "      <td>0.000</td>\n",
              "      <td>u</td>\n",
              "      <td>g</td>\n",
              "      <td>w</td>\n",
              "      <td>v</td>\n",
              "      <td>1.25</td>\n",
              "      <td>t</td>\n",
              "      <td>t</td>\n",
              "      <td>1</td>\n",
              "      <td>f</td>\n",
              "      <td>g</td>\n",
              "      <td>00202</td>\n",
              "      <td>0</td>\n",
              "      <td>+</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>a</td>\n",
              "      <td>58.67</td>\n",
              "      <td>4.460</td>\n",
              "      <td>u</td>\n",
              "      <td>g</td>\n",
              "      <td>q</td>\n",
              "      <td>h</td>\n",
              "      <td>3.04</td>\n",
              "      <td>t</td>\n",
              "      <td>t</td>\n",
              "      <td>6</td>\n",
              "      <td>f</td>\n",
              "      <td>g</td>\n",
              "      <td>00043</td>\n",
              "      <td>560</td>\n",
              "      <td>+</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>a</td>\n",
              "      <td>24.50</td>\n",
              "      <td>0.500</td>\n",
              "      <td>u</td>\n",
              "      <td>g</td>\n",
              "      <td>q</td>\n",
              "      <td>h</td>\n",
              "      <td>1.50</td>\n",
              "      <td>t</td>\n",
              "      <td>f</td>\n",
              "      <td>0</td>\n",
              "      <td>f</td>\n",
              "      <td>g</td>\n",
              "      <td>00280</td>\n",
              "      <td>824</td>\n",
              "      <td>+</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>b</td>\n",
              "      <td>27.83</td>\n",
              "      <td>1.540</td>\n",
              "      <td>u</td>\n",
              "      <td>g</td>\n",
              "      <td>w</td>\n",
              "      <td>v</td>\n",
              "      <td>3.75</td>\n",
              "      <td>t</td>\n",
              "      <td>t</td>\n",
              "      <td>5</td>\n",
              "      <td>t</td>\n",
              "      <td>g</td>\n",
              "      <td>00100</td>\n",
              "      <td>3</td>\n",
              "      <td>+</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>b</td>\n",
              "      <td>20.17</td>\n",
              "      <td>5.625</td>\n",
              "      <td>u</td>\n",
              "      <td>g</td>\n",
              "      <td>w</td>\n",
              "      <td>v</td>\n",
              "      <td>1.71</td>\n",
              "      <td>t</td>\n",
              "      <td>f</td>\n",
              "      <td>0</td>\n",
              "      <td>f</td>\n",
              "      <td>s</td>\n",
              "      <td>00120</td>\n",
              "      <td>0</td>\n",
              "      <td>+</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  0      1      2  3  4  5  6     7  8  9   10 11 12     13   14 15\n",
              "0  b  30.83  0.000  u  g  w  v  1.25  t  t   1  f  g  00202    0  +\n",
              "1  a  58.67  4.460  u  g  q  h  3.04  t  t   6  f  g  00043  560  +\n",
              "2  a  24.50  0.500  u  g  q  h  1.50  t  f   0  f  g  00280  824  +\n",
              "3  b  27.83  1.540  u  g  w  v  3.75  t  t   5  t  g  00100    3  +\n",
              "4  b  20.17  5.625  u  g  w  v  1.71  t  f   0  f  s  00120    0  +"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 451
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSIXKuSlCXdt"
      },
      "source": [
        "Some info about this dataframe,because this is real data, the names of the columns werent included, but we can still guess about some of them, maybe age, gender, social status, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "989pkSSRC2OB",
        "outputId": "5b9d928b-f61c-4466-84ed-8bfee59c9a63"
      },
      "source": [
        "df.dtypes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      object\n",
              "1      object\n",
              "2     float64\n",
              "3      object\n",
              "4      object\n",
              "5      object\n",
              "6      object\n",
              "7     float64\n",
              "8      object\n",
              "9      object\n",
              "10      int64\n",
              "11     object\n",
              "12     object\n",
              "13     object\n",
              "14      int64\n",
              "15     object\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 452
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjABRxy2Uw4C"
      },
      "source": [
        "*The datatypes and values acording to the source*\n",
        " \n",
        "* A1: b, a.\n",
        "* A2: continuous.\n",
        "* A3: continuous.\n",
        "* A4: u, y, l, t.\n",
        "* A5: g, p, gg.\n",
        "* A6: c, d, cc, i, j, k, m, r, q, w, x, e, aa, ff.\n",
        "* A7: v, h, bb, j, n, z, dd, ff, o.\n",
        "* A8: continuous.\n",
        "* A9: t, f.\n",
        "* A10: t, f.\n",
        "* A11: continuous.\n",
        "* A12: t, f.\n",
        "* A13: g, p, s.\n",
        "* A14: continuous.\n",
        "* A15: continuous.\n",
        "* A16: +,- (class attribute)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mEIsIheDwiI",
        "outputId": "c2ad580f-750e-4474-cdcb-a9436018ebcd"
      },
      "source": [
        "# Most of the columns are objects, there are 2 floats and one integrer\n",
        "# lets see the unique values of each column\n",
        "# lets drop first the numeric columns in a df, df2\n",
        "# I cant transform the datatype of the column to float due to the missing values it contains with the \"?\" sign\n",
        "# df[1]= df[1].astype(\"float64\")\n",
        "print(df[2].dtypes)\n",
        "df2 = df.select_dtypes(exclude=['int64',\"float64\"])\n",
        "# Excluding the numeric columns, stored as objects due to the ? sign\n",
        "df2.drop(columns=[1,13], axis=1, inplace=True)\n",
        "# a for loop to iterate between columns and print its unique values\n",
        "for column in df2:\n",
        "    unique_values = df2[column].unique()\n",
        "    print(column, \"unique values:\", unique_values)\n",
        "# The missing values are assigned with a \"?\" sign"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "float64\n",
            "0 unique values: ['b' 'a' '?']\n",
            "3 unique values: ['u' 'y' '?' 'l']\n",
            "4 unique values: ['g' 'p' '?' 'gg']\n",
            "5 unique values: ['w' 'q' 'm' 'r' 'cc' 'k' 'c' 'd' 'x' 'i' 'e' 'aa' 'ff' 'j' '?']\n",
            "6 unique values: ['v' 'h' 'bb' 'ff' 'j' 'z' '?' 'o' 'dd' 'n']\n",
            "8 unique values: ['t' 'f']\n",
            "9 unique values: ['t' 'f']\n",
            "11 unique values: ['f' 't']\n",
            "12 unique values: ['g' 's' 'p']\n",
            "15 unique values: ['+' '-']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:4174: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwbKS-JjDInC",
        "outputId": "c768ce7d-688f-4289-d935-cd48bbae49e2"
      },
      "source": [
        "# lets replace the ? with np.nan\n",
        "df = df.replace(\"?\", np.nan)\n",
        "# And the + and - from the column 15 to 1 and 0\n",
        "df[15] = df[15].replace(\"+\", 1)\n",
        "df[15] = df[15].replace(\"-\", 0)\n",
        "\n",
        "# now i can correct their datatype so it matches with the source info\n",
        "df[1]= df[1].astype(\"float64\")\n",
        "df[13]= df[13].astype(\"float64\")\n",
        "# check for Na's\n",
        "def na_status(df):\n",
        "  total = df.isnull().sum().sort_values(ascending=False)\n",
        "  percent_1 = df.isnull().sum()/df.isnull().count()*100\n",
        "  percent_2 = (round(percent_1, 1)).sort_values(ascending=False)\n",
        "  missing_data = pd.concat([total, percent_2], axis=1, keys=['Total NaN', 'NaN %'])\n",
        "  print(missing_data.head(10))\n",
        "na_status(df)\n",
        "# We have up to almost 2% of missing values in some columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    Total NaN  NaN %\n",
            "13         13    1.9\n",
            "1          12    1.7\n",
            "0          12    1.7\n",
            "6           9    1.3\n",
            "5           9    1.3\n",
            "4           6    0.9\n",
            "3           6    0.9\n",
            "15          0    0.0\n",
            "14          0    0.0\n",
            "12          0    0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpHrLsC-ZCJi",
        "outputId": "b8e433b1-42ad-44d0-ae33-3eec339b2541"
      },
      "source": [
        "# Lets impute the missing values using iterative imputator\n",
        "# First i need to encode the values of the dataframe, using label encoder from sklearn\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# Then i create an instance for the label encoder\n",
        "encoder = LabelEncoder()\n",
        "\n",
        "# Using df.apply, i used fit and transform in each column, keeping the Na's using not null from series,\n",
        "# otherwise it would've encoded those Na's\n",
        "df = df.apply(lambda series: pd.Series(\n",
        "    encoder.fit_transform(series[series.notnull()]),\n",
        "    index=series[series.notnull()].index\n",
        "))\n",
        "\n",
        "# Lets revise that the encoder kept the missing values\n",
        "# I remove this numeric columns, these dont contain any missing values \n",
        "columns_numeric = df[[1,2,7,14]]\n",
        "df2 = df.drop(columns=columns_numeric, axis=1)\n",
        "# using the same for loop as before to check the unique values\n",
        "for column in df2:\n",
        "    unique_values = df2[column].unique()\n",
        "    print(column, \"unique values:\", unique_values)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 unique values: [ 1.  0. nan]\n",
            "3 unique values: [ 1.  2. nan  0.]\n",
            "4 unique values: [ 0.  2. nan  1.]\n",
            "5 unique values: [12. 10.  9. 11.  2.  8.  1.  3. 13.  6.  4.  0.  5.  7. nan]\n",
            "6 unique values: [ 7.  3.  0.  2.  4.  8. nan  6.  1.  5.]\n",
            "8 unique values: [1 0]\n",
            "9 unique values: [1 0]\n",
            "10 unique values: [ 1  6  0  5  7 10  3 17  2  9  8 15 11 12 21 20  4 19 22 14 16 13 18]\n",
            "11 unique values: [0 1]\n",
            "12 unique values: [0 2 1]\n",
            "13 unique values: [ 68.  11.  96.  31.  37. 115.  54.  23.  62.  15.  39.  90.   0. 105.\n",
            " 127.  29.  67. 100.  47. 150.  56. 138. 158.   8.  84.  19. 143. 103.\n",
            "  74. 149. 129.  83.  52. 162.  85. 154. 152. 134.  nan 167. 140.  44.\n",
            "  28. 116.  97. 166.  65.  35.  58.  92.  55.  21.  49.  60. 106.  73.\n",
            " 131.  94. 121. 130. 112.  69.  10.  63. 128. 139.  27.  17. 126. 125.\n",
            "   3.   7.  32. 136. 118.   5.   2.  40. 151.  66.  46. 122.  13.  14.\n",
            " 123.  48.  36.  16.  72.  80.  51.   4.  79. 153.  87. 148.  75.  25.\n",
            "  20.  38. 146.  43.  42.  99.  50.  93. 109.  33. 163. 141.  82.  57.\n",
            " 168. 132. 144. 110. 147. 160. 120.  95.  76. 113.  45.  61. 159. 156.\n",
            " 169.  78.  71.  34. 114.  12.  18.  26.  59.   1.  24. 142.  88.   6.\n",
            "  91. 108. 104.  98. 119.  30.  86. 164. 157.  41. 155. 161.  53.  70.\n",
            " 124. 107. 135. 165. 137.   9.  64. 101. 111.  77. 117.  22.  81. 102.\n",
            " 133. 145.  89.]\n",
            "15 unique values: [1 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIGDlVl1u3eo",
        "outputId": "9d27b186-57f0-4ecb-a0e7-70dfd288d1fa"
      },
      "source": [
        "# We can see that the missing values where kept and not encoded\n",
        "# Now the imputation can take place\n",
        "\n",
        "# To impute the values i need the iterative imputer and the enable iterative imputer from sklearn\n",
        "from sklearn.experimental import enable_iterative_imputer \n",
        "from sklearn.impute import IterativeImputer\n",
        "# an instance of iterative imputer with random state 20\n",
        "imp = IterativeImputer(random_state=20)\n",
        "# The imputatior returns an array, but i want to check before if there are any nas left, using my function na_status\n",
        "# the column names wont be kept, because there arent any\n",
        "df = pd.DataFrame(imp.fit_transform(df))\n",
        "na_status(df)\n",
        "# There arent any Na's left"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    Total NaN  NaN %\n",
            "15          0    0.0\n",
            "14          0    0.0\n",
            "13          0    0.0\n",
            "12          0    0.0\n",
            "11          0    0.0\n",
            "10          0    0.0\n",
            "9           0    0.0\n",
            "8           0    0.0\n",
            "7           0    0.0\n",
            "6           0    0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gje7RKGywENz"
      },
      "source": [
        "# Data split and preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, \n",
        "\n",
        "# An instance of scaler \n",
        "scaler = StandardScaler()\n",
        "\n",
        "# The features\n",
        "X = df.drop(columns=[15], axis=1)\n",
        "# The target feature\n",
        "y = df[15]\n",
        "\n",
        "# The train, test split, leaving 35% of the data for testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.35, random_state=42)\n",
        "# Scaling the data in the training and testing data\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.fit_transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqnpVnj85WmK"
      },
      "source": [
        "# To predict the aproval i will use an ML stacked model\n",
        "# The models i will be using \n",
        "from sklearn import svm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neural_network import MLPClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQlcZIkL5nga",
        "outputId": "d908b7d5-9f82-4dfb-c84d-ff9faf148211"
      },
      "source": [
        "# Defining and fitting the models\n",
        "#SVM\n",
        "svm= svm.SVC()\n",
        "svm.fit(X_train_scaled, y_train)\n",
        "\n",
        "#MLP\n",
        "nnet = MLPClassifier(max_iter=1500)\n",
        "nnet.fit(X_train_scaled, y_train)\n",
        "\n",
        "#RFC\n",
        "rforest = RandomForestClassifier()\n",
        "rforest.fit(X_train_scaled, y_train)\n",
        "\n",
        "#KNN\n",
        "knn = KNeighborsClassifier()\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Dtree\n",
        "tree = DecisionTreeClassifier()\n",
        "tree.fit(X_train_scaled,y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
              "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                       random_state=None, splitter='best')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 459
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpV4MRE-55A_"
      },
      "source": [
        "#Now to assemble all the ML models\n",
        "from sklearn.ensemble import StackingClassifier \n",
        "# And the final model, will be logistic regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# The instance of logistic regression\n",
        "logreg= LogisticRegression()\n",
        "\n",
        "# list of estimators\n",
        "estimators = [\n",
        "              ('nnet',nnet),\n",
        "              ('tree', tree),\n",
        "              ('svm', svm),\n",
        "              ('knn', knn),\n",
        "              ('rforest', rforest)\n",
        "]\n",
        "# The stacked model, created using stacking classifier from sklearn\n",
        "# the list of estimators and the final estimator are assigned as previously stated\n",
        "stack_model = StackingClassifier(estimators = estimators, final_estimator = logreg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iADeGlo86XX0"
      },
      "source": [
        "# fitting the stacked model\n",
        "stack_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# creating predictions on X_train and X_test, to test the performance\n",
        "y_train_pred = stack_model.predict(X_train_scaled)\n",
        "y_test_pred = stack_model.predict(X_test_scaled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0b8pi-E6e7X",
        "outputId": "1ce0f9f8-a64f-4d00-872b-51c6b6f76d46"
      },
      "source": [
        "#the metrics to evaluate the model performance\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Training set model performance\n",
        "stack_model_train_accuracy = accuracy_score(y_train, y_train_pred) # Calculate Accuracy\n",
        "stack_model_train_mcc = matthews_corrcoef(y_train, y_train_pred) # Calculate MCC\n",
        "stack_model_train_f1 = f1_score(y_train, y_train_pred, average='weighted') # Calculate F1-score\n",
        "\n",
        "# Test set model performance\n",
        "stack_model_test_accuracy = accuracy_score(y_test, y_test_pred) # Calculate Accuracy\n",
        "stack_model_test_mcc = matthews_corrcoef(y_test, y_test_pred) # Calculate MCC\n",
        "stack_model_test_f1 = f1_score(y_test, y_test_pred, average='weighted') # Calculate F1-score\n",
        "\n",
        "# Printing the results\n",
        "print('Model performance for Training set')\n",
        "print('- Accuracy: %s' % stack_model_train_accuracy)\n",
        "print('- MCC: %s' % stack_model_train_mcc)\n",
        "print('- F1 score: %s' % stack_model_train_f1)\n",
        "print('----------------------------------')\n",
        "print('Model performance for Test set')\n",
        "print('- Accuracy: %s' % stack_model_test_accuracy)\n",
        "print('- MCC: %s' % stack_model_test_mcc)\n",
        "print('- F1 score: %s' % stack_model_test_f1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model performance for Training set\n",
            "- Accuracy: 0.9732142857142857\n",
            "- MCC: 0.9456969696969697\n",
            "- F1 score: 0.9732142857142857\n",
            "----------------------------------\n",
            "Model performance for Test set\n",
            "- Accuracy: 0.8801652892561983\n",
            "- MCC: 0.758176479685776\n",
            "- F1 score: 0.8802127812583885\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sN_TECStBzwS"
      },
      "source": [
        "The model reached an accuracy of 88% and an F1-score of 88% too, but according to the Mathews coefficient the model seems to generalize and predict with a lesser quality on new data; it could also be that the model is overfitting, due to the fact that the training data got scores above 94% for all the three measures."
      ]
    }
  ]
}